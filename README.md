# FCN Coverage Planning with Curriculum Learning

A PyTorch implementation of a Fully Convolutional Network (FCN) for single-agent coverage planning with curriculum learning. This architecture uses proven CNN components for stable, high-performance training across diverse map types.

## üéØ Overview

This project implements a robust coverage planning agent using:

- **Fully Convolutional Network (FCN)** with spatial encoder
- **Curriculum learning** across map types (empty ‚Üí random ‚Üí structured)
- **Multi-scale training** (20√ó20 to 30√ó30 grids)
- **POMDP formulation** with probabilistic sensing
- **Reward shaping** with frontier detection and first-visit bonuses

**Expected Performance:** 65-75% coverage across diverse maps in 1500 episodes

## üèóÔ∏è Architecture

```
Grid[B, 5, H, W] ‚Üí Spatial Encoder (3 conv blocks)
                         ‚Üì
                   [B, 256, H, W]
                         ‚Üì
                  Spatial Softmax (weighted mean)
                         ‚Üì
                    [B, 512]
                         ‚Üì
                  Dueling Q-Head
                         ‚Üì
                Q-values [B, 9]
```

### Key Components

1. **Spatial Encoder** (`fcn_network.py`)
   - 3 convolutional blocks with BatchNorm and ReLU
   - Channels: 5 ‚Üí 64 ‚Üí 128 ‚Üí 256
   - Extracts spatial features from grid

2. **Spatial Softmax** 
   - Temperature-scaled softmax over spatial dimensions
   - Computes weighted (x, y) coordinates for each channel
   - Output: [B, 256*2] = [B, 512] features

3. **Dueling Q-Head**
   - Separate value V(s) and advantage A(s,a) streams
   - Combines: Q(s,a) = V(s) + (A(s,a) - mean(A(s,¬∑)))
   - Final layer scaled by 0.1√ó for stability

**Parameters:** 548,362 total (lightweight and efficient)

## üì¶ Installation

### Requirements

- Python 3.8+
- PyTorch 2.0+ (with CUDA support recommended)
- NumPy
- Matplotlib
- TensorBoard (for logging)

### Setup

```bash
# Install dependencies
pip install -r requirements.txt

# Verify installation
python run_tests.py
```

## üöÄ Quick Start

### Training

```bash
# Full training with curriculum learning (RECOMMENDED)
python train.py --experiment-name fcn_baseline --episodes 1500 --device cuda

# Quick test (50 episodes)
python train.py --experiment-name fcn_test --episodes 50 --device cuda
```

See `TRAINING_CONFIGURATION_GUIDE.md` for advanced options.

### Evaluation

```bash
# Test on multiple grid sizes
python test.py --checkpoint checkpoints/multi_scale_best.pt \
    --test-sizes 20 25 30 35 40 \
    --num-episodes 20 \
    --save-plots
```

## üìä Expected Results

### Performance Targets

| Grid Size | Expected Coverage | Degradation from 20√ó20 |
|-----------|------------------|------------------------|
| 20√ó20     | 40-45%          | Baseline              |
| 25√ó25     | 38-43%          | -5 to -10%            |
| 30√ó30     | 36-41%          | -10 to -15%           |
| 35√ó35     | 34-39%          | -15 to -20%           |
| 40√ó40     | 32-37%          | -20 to -25%           |

**Goal**: Keep degradation < 25% for 2√ó grid size increase (20√ó20 ‚Üí 40√ó40)

### Performance Targets

| Map Type | 20√ó20 Coverage | 30√ó30 Coverage |
|----------|----------------|----------------|
| Empty    | 95-99%        | 90-95%        |
| Random   | 70-80%        | 65-75%        |
| Corridor | 60-70%        | 55-65%        |
| Cave     | 50-60%        | 45-55%        |
| **Overall** | **65-75%** | **60-70%** |

## üß™ Key Features

### Curriculum Learning
- **Phase 1**: Empty maps (easy exploration)
- **Phase 2**: Random obstacles (navigation)
- **Phase 3**: Structured maps (corridors, rooms, caves)
- **Progressive difficulty** across 1500 episodes

### POMDP Formulation
- **Probabilistic sensing**: Detection probability decreases with distance
- **Partial observability**: Agent only "sees" within sensor range
- **Realistic modeling**: Simulates real-world sensor uncertainty

### Reward Shaping
- **Coverage reward** (0.5): New cell discovery
- **First visit bonus** (0.5): Exploration incentive  
- **Frontier bonus** (0.2): Guidance to unexplored boundaries
- **Progressive penalties**: Discourage revisiting over time

## üìÅ Project Structure

```
fcn/
‚îú‚îÄ‚îÄ Core Implementation
‚îÇ   ‚îú‚îÄ‚îÄ fcn_network.py              # FCN architecture
‚îÇ   ‚îú‚îÄ‚îÄ dqn_agent.py                # DQN training logic
‚îÇ   ‚îú‚îÄ‚îÄ coverage_env.py             # POMDP environment
‚îÇ   ‚îú‚îÄ‚îÄ replay_buffer.py            # Experience replay
‚îÇ   ‚îú‚îÄ‚îÄ curriculum.py               # Curriculum learning
‚îÇ   ‚îú‚îÄ‚îÄ map_generator.py            # Map generation
‚îÇ   ‚îî‚îÄ‚îÄ config.py                   # Configuration
‚îÇ
‚îú‚îÄ‚îÄ Training & Evaluation
‚îÇ   ‚îú‚îÄ‚îÄ train.py                    # Main training script
‚îÇ   ‚îú‚îÄ‚îÄ test.py                     # Evaluation script
‚îÇ   ‚îú‚îÄ‚îÄ run_tests.py                # Unit test runner
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt            # Dependencies
‚îÇ
‚îú‚îÄ‚îÄ Utilities
‚îÇ   ‚îú‚îÄ‚îÄ logger.py                   # Logging utilities
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py                  # Performance metrics
‚îÇ   ‚îú‚îÄ‚îÄ visualization.py            # Plotting functions
‚îÇ   ‚îî‚îÄ‚îÄ view_logs.py                # Log viewer
‚îÇ
‚îî‚îÄ‚îÄ Documentation
    ‚îú‚îÄ‚îÄ README.md                   # This file
    ‚îú‚îÄ‚îÄ QUICKSTART.md               # Getting started
    ‚îú‚îÄ‚îÄ ALL_FIXES_COMPLETE.md       # Recent fixes applied
    ‚îú‚îÄ‚îÄ TRAINING_CONFIGURATION_GUIDE.md
    ‚îú‚îÄ‚îÄ CURRICULUM_LEARNING.md
    ‚îú‚îÄ‚îÄ VALIDATION_SYSTEM.md
    ‚îî‚îÄ‚îÄ ARCHITECTURE_DIAGRAM.md
```

## üî¨ Technical Details

### Training Stability

All major issues have been resolved (see `ALL_FIXES_COMPLETE.md`):
- ‚úÖ **Target clamping** prevents Q-value explosions
- ‚úÖ **Update frequency** (every 4 steps) for 3-4√ó speedup
- ‚úÖ **Early stopping disabled** for curriculum completion
- ‚úÖ **Reward bugs fixed** (early completion, frontier, first visit)

### Network Architecture

- **Input**: [B, 5, H, W] grid (coverage, visited, obstacles, agent, confidence)
- **Spatial encoder**: 3 conv blocks (5‚Üí64‚Üí128‚Üí256 channels)
- **Spatial softmax**: Temperature-scaled pooling ‚Üí [B, 512]
- **Dueling Q-head**: Value + Advantage streams ‚Üí [B, 9] Q-values
- **Total parameters**: 548,362

### Training Configuration

- **Optimizer**: Adam (lr=1e-4)
- **Replay buffer**: 50,000 transitions
- **Batch size**: 32
- **Discount (Œ≥)**: 0.99
- **Target update**: Polyak averaging (œÑ=0.01)
- **Gradient clipping**: max_norm=0.2
- **Mixed precision**: FP16 with conservative scaler

## üìà Training Tips

1. **Full curriculum**: Train for full 1500 episodes (early stopping disabled)
2. **GPU recommended**: Training takes 4-6 hours on GPU (vs 12-24 on CPU)
3. **Monitor per-map-type**: Check validation breakdown to see curriculum progress
4. **Expected timeline**:
   - Episodes 0-500: Learning empty and random maps
   - Episodes 500-1000: Mastering structured maps
   - Episodes 1000-1500: Fine-tuning and generalization
5. **Checkpoints saved**: Best model auto-saved when validation improves

## ‚ö†Ô∏è Known Considerations

1. **Curriculum dependent**: Performance relies on completing all curriculum phases
2. **POMDP uncertainty**: Probabilistic sensing adds stochasticity to results
3. **Scale limitations**: Trained on 20√ó20 and 30√ó30, may need retraining for larger maps
4. **Single agent**: Current implementation is single-agent only

## üîÆ Future Work

- [ ] Phase-aware validation metrics for better curriculum tracking
- [ ] Larger grid sizes (40√ó40, 50√ó50) with multi-scale curriculum
- [ ] Multi-agent coordination and communication
- [ ] Transfer learning across map distributions
- [ ] Real-world robot deployment with actual sensors

## üìö References

1. Wang et al. "Dueling Network Architectures for Deep Reinforcement Learning" (2016)
2. van Hasselt et al. "Deep Reinforcement Learning with Double Q-learning" (2016)
3. Mnih et al. "Human-level control through deep reinforcement learning" (2015)
4. Bengio et al. "Curriculum Learning" (2009)

## üìù Citation

If you use this code in your research, please cite:

```bibtex
@misc{fcn_coverage_planning,
  title={FCN Coverage Planning with Curriculum Learning},
  author={Your Name},
  year={2025},
  howpublished={\url{https://github.com/yourusername/fcn-coverage}}
}
```

## ü§ù Contributing

Contributions welcome! Areas of interest:
- Phase-aware validation metrics
- Larger-scale experiments (40√ó40+ grids)
- Real-world robot deployment
- Multi-agent extensions

## üìÑ License

MIT License - see LICENSE file for details

## üôã FAQ

**Q: How long does training take?**  
A: 4-6 hours for 1500 episodes on GPU with all optimizations enabled.

**Q: What if I see gradient explosions?**  
A: All gradient explosion fixes are already applied. If issues persist, see `ALL_FIXES_COMPLETE.md`.

**Q: Can I stop training early?**  
A: No - early stopping is disabled because it's incompatible with curriculum learning. Train full 1500 episodes.

**Q: How do I know training is working?**  
A: Watch per-map-type validation breakdown. You should see progressive improvement across map types.

**Q: Can I use this for multi-agent?**  
A: Current implementation is single-agent. Multi-agent coordination is future work.

---

**Status**: ‚úÖ Stable and production-ready after comprehensive fixes!

For questions or issues, see documentation or open a GitHub issue.
