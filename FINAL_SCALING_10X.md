# Final Reward Scaling: 10Ã— Adjustment

## Issue: Vanishing Gradients
Initial scaling (0.02, 0.01, etc.) was **too small**:
- Episode returns: ~+10
- Single-step rewards: ~0.05
- **Risk:** Vanishing gradients during backpropagation

## Solution: 10Ã— Scaling
All reward components scaled up by 10Ã—:

### New Values (10Ã— Scaled)
```python
# Primary rewards
coverage_reward = 0.2              # was 0.02 â†’ ~68 per episode
coverage_confidence_weight = 0.1   # was 0.01 â†’ ~34 per episode
early_completion_bonus = 20.0      # was 2.0
time_bonus_per_step_saved = 0.1    # was 0.01 â†’ ~17 time bonus
frontier_bonus = 0.5               # was 0.05 â†’ ~25 per episode

# Penalties
step_penalty = -0.05               # was -0.005 â†’ ~-9 per episode
rotation_penalty = -0.1/-0.2/-0.5  # was -0.01/-0.02/-0.05
revisit_penalty = -0.5             # was -0.05 â†’ ~-15 per episode
collision_penalty = -5.0           # was -0.5 â†’ ~-50 per episode
stay_penalty = -1.0                # was -0.1 â†’ ~-5 per episode
```

## Expected Dynamics

### Episode Returns (Ideal Trajectory)
```
POSITIVE:
  Coverage:        +68.0
  Confidence:      +34.0
  Completion:      +37.0
  Frontier:        +25.0
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total:          +164.0

NEGATIVE:
  Collision:       -60.0  (12 collisions early training)
  Revisit:         -15.0
  Rotation:        -12.0
  Step:             -9.0
  STAY:             -5.0
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total:          -101.0

NET RETURN:        +63.0
```

### Training Phases
- **Random policy:** -200 to +20 (mostly negative due to collisions)
- **Early training:** -50 to +50 (learning to avoid collisions)
- **Mid training:** +20 to +80 (improving coverage)
- **Late training:** +50 to +100 (efficient coverage)

### Gradient Health
- **Q-values:** ~6,300 (with Î³=0.99)
- **TD errors:** ~60-100 per step
- **Gradient magnitude:** Healthy range for Adam optimizer
- **No vanishing:** Large enough for signal propagation
- **No explosion:** Still 35Ã— smaller than original (3,500)

## Comparison

| Metric | Original (Broken) | First Fix (Too Small) | Final (10Ã— Scaled) |
|--------|-------------------|----------------------|-------------------|
| Episode return | +3,500 | +10 | +100 |
| Q-values | ~350,000 | ~1,000 | ~10,000 |
| Gradient risk | Explosion ğŸ”¥ | Vanishing â„ï¸ | Healthy âœ… |
| Single-step reward | ~10-50 | ~0.05 | ~0.5 |
| Coverage per episode | +3,400 | +6.8 | +68 |
| Collision penalty | -50 (never triggered) | -6.0 | -60 |

## Rationale

### Why 10Ã— is Optimal

1. **Gradient strength:** Single-step rewards ~0.5, strong enough for BP
2. **Q-value range:** ~10,000 is ideal for DQN (not too big, not too small)
3. **TD error magnitude:** ~100 works well with Adam (lr=0.0003)
4. **Reward interpretation:** Hundreds range is intuitive
5. **Still balanced:** Penalties are 62% of rewards (same ratio maintained)

### Historical Context
- **Original:** Coverage dominates (91% of reward) â†’ gradient explosion
- **Too small (1Ã—):** Everything shrinks â†’ vanishing gradients
- **Just right (10Ã—):** Balanced and strong â†’ healthy gradients

## Summary

âœ… **Episode returns in hundreds** (avoiding vanishing gradients)
âœ… **Penalties still 62% of rewards** (balance maintained)  
âœ… **Q-values ~10,000** (ideal for DQN)
âœ… **No explosion risk** (35Ã— smaller than original)

**This is the Goldilocks zone!** ğŸ¯
